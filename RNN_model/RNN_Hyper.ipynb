{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS OPTIMIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECTIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of this notebook is finding the optimal parameters for the RNN Model.\n",
    "\n",
    "Among the different hyperparameters the following values are considered:\n",
    "    \n",
    "    - Learning rate\n",
    "    - Momentum\n",
    "    - Distance\n",
    "    - Batch_size\n",
    "    - Initializer\n",
    "    - Optimizer\n",
    "    - Hidden Layers\n",
    "    - Hidden Neurons\n",
    "\n",
    "Two techniques will be used to optimize those values:\n",
    "\n",
    "    -Grid Search\n",
    "    -Random Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import keras\n",
    "import keras.backend as Kback\n",
    "from keras import layers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, SimpleRNN\n",
    "from keras.layers.core import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "model_path = 'RNN_Hyp_1.h5'\n",
    "\n",
    "# RNN_regression_PCA : First model with the highest accuracy \n",
    "# RNN_regression_PCA_1: Increasing the variability explained by PCA from 0.95 to 0.98\n",
    "# RNN_regression_PCA_2: Reducing the sequence from 50 to 30\n",
    "# RNN_regression_PCA_3: Increasing the sequence from 50 to 60\n",
    "\n",
    "import random\n",
    "random.seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "#Data ingestion & processing\n",
    "######\n",
    "\n",
    "train_df = pd.read_csv(\"C:/Users/eduardo.tadeo/Documents/Master Thesis/Datasets/CMAPSSData/train_FD001.txt\", delimiter = ',')\n",
    "test_df  = pd.read_csv(\"C:/Users/eduardo.tadeo/Documents/Master Thesis/Datasets/CMAPSSData/test_FD001.txt\", delimiter = ',')\n",
    "RUL_test = pd.read_csv(\"C:/Users/eduardo.tadeo/Documents/Master Thesis/Datasets/CMAPSSData/RUL_FD001.txt\")\n",
    "\n",
    "#Unuseful columns deleting \n",
    "\n",
    "train_df.drop(['T_EGT', 'SmFan', 'SmLPC', 'SmHPC'], axis = 1, inplace = True)\n",
    "\n",
    "test_df.drop(['T_EGT', 'SmFan', 'SmLPC', 'SmHPC'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "###TRAIN DATA SET###\n",
    "\n",
    "# Standarization for both sensor variables and operational conditions\n",
    "\n",
    "train_df['Cycle_norm'] = train_df['Cycle']\n",
    "cols_normalize = train_df.columns.difference(['Unit','Cycle','UL','RUL','UL_30','UL_50', 'UL_75'])\n",
    "scaler = StandardScaler()\n",
    "scaled_train = scaler.fit_transform(train_df[cols_normalize])\n",
    "norm_train_df = pd.DataFrame(scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)\n",
    "\n",
    "# Principal Component Analysis Dimensionality Reduction\n",
    "\n",
    "pca = PCA(.95)\n",
    "\n",
    "pca.fit(norm_train_df)\n",
    "transformed_train = pca.transform(scaled_train)\n",
    "\n",
    "df_transformed_train = pd.DataFrame(transformed_train, index = train_df.index)\n",
    "\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(df_transformed_train)\n",
    "train_df = join_df.reindex()\n",
    "\n",
    "\n",
    "# Data labeling - Remaining Useful LIfe (RUL) --> Time to failure\n",
    "\n",
    "RUL = pd.DataFrame(train_df.groupby('Unit')['Cycle'].max()).reset_index()\n",
    "RUL.columns = ['Unit','UL']\n",
    "train_df = train_df.merge(RUL, on = ['Unit'], how = 'left')\n",
    "train_df['RUL'] = train_df['UL'] - train_df['Cycle']\n",
    "\n",
    "\n",
    "# Data labeling - UL_30 - UL_50 - UL_75 --> Labeling to predict if the turbine is going to fail on les than 30, 50 or 75 cycles\n",
    "\n",
    "train_df['UL_30'] = np.where(train_df['RUL'] <= 30, 1, 0)\n",
    "train_df['UL_50'] = np.where(train_df['RUL'] <= 50, 1, 0)\n",
    "train_df['UL_75'] = np.where(train_df['RUL'] <= 75, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Cycle  Unit         0         1         2         3         4         5  \\\n0      1     1 -3.240845 -0.559371 -1.176731  0.489354 -0.674640  0.769576   \n1      2     1 -2.669143 -0.940659 -0.137753  1.167902 -0.706549  0.927689   \n2      3     1 -3.260073 -0.665652 -0.528376 -2.116760  0.358836  0.752253   \n3      4     1 -3.655610 -0.931102  0.253664  0.143214 -0.343321  0.521816   \n4      5     1 -2.716606 -0.510696 -1.027479 -0.294257 -0.363062  0.651352   \n\n          6         7         8         9        10        11        12   UL  \\\n0 -0.669039  0.631733 -0.421074  0.039968  0.643934  0.289824  0.222595  192   \n1 -0.789666  0.287941 -0.121802 -0.306724  0.658158  0.108694 -0.231444  192   \n2 -0.429145 -1.129094 -0.590819  0.269416  0.098841 -0.111126  0.240092  192   \n3  0.534238 -0.070660  0.000428 -0.105096 -1.125582  0.385607  0.633467  192   \n4  0.902064  0.330613 -0.084591 -0.475373  0.228016 -0.122698  0.070388  192   \n\n   RUL  UL_30  UL_50  UL_75  \n0  191      0      0      0  \n1  190      0      0      0  \n2  189      0      0      0  \n3  188      0      0      0  \n4  187      0      0      0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cycle</th>\n      <th>Unit</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>UL</th>\n      <th>RUL</th>\n      <th>UL_30</th>\n      <th>UL_50</th>\n      <th>UL_75</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-3.240845</td>\n      <td>-0.559371</td>\n      <td>-1.176731</td>\n      <td>0.489354</td>\n      <td>-0.674640</td>\n      <td>0.769576</td>\n      <td>-0.669039</td>\n      <td>0.631733</td>\n      <td>-0.421074</td>\n      <td>0.039968</td>\n      <td>0.643934</td>\n      <td>0.289824</td>\n      <td>0.222595</td>\n      <td>192</td>\n      <td>191</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>-2.669143</td>\n      <td>-0.940659</td>\n      <td>-0.137753</td>\n      <td>1.167902</td>\n      <td>-0.706549</td>\n      <td>0.927689</td>\n      <td>-0.789666</td>\n      <td>0.287941</td>\n      <td>-0.121802</td>\n      <td>-0.306724</td>\n      <td>0.658158</td>\n      <td>0.108694</td>\n      <td>-0.231444</td>\n      <td>192</td>\n      <td>190</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>-3.260073</td>\n      <td>-0.665652</td>\n      <td>-0.528376</td>\n      <td>-2.116760</td>\n      <td>0.358836</td>\n      <td>0.752253</td>\n      <td>-0.429145</td>\n      <td>-1.129094</td>\n      <td>-0.590819</td>\n      <td>0.269416</td>\n      <td>0.098841</td>\n      <td>-0.111126</td>\n      <td>0.240092</td>\n      <td>192</td>\n      <td>189</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>-3.655610</td>\n      <td>-0.931102</td>\n      <td>0.253664</td>\n      <td>0.143214</td>\n      <td>-0.343321</td>\n      <td>0.521816</td>\n      <td>0.534238</td>\n      <td>-0.070660</td>\n      <td>0.000428</td>\n      <td>-0.105096</td>\n      <td>-1.125582</td>\n      <td>0.385607</td>\n      <td>0.633467</td>\n      <td>192</td>\n      <td>188</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>-2.716606</td>\n      <td>-0.510696</td>\n      <td>-1.027479</td>\n      <td>-0.294257</td>\n      <td>-0.363062</td>\n      <td>0.651352</td>\n      <td>0.902064</td>\n      <td>0.330613</td>\n      <td>-0.084591</td>\n      <td>-0.475373</td>\n      <td>0.228016</td>\n      <td>-0.122698</td>\n      <td>0.070388</td>\n      <td>192</td>\n      <td>187</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(20631, 25) (13096, 25)\n"
    }
   ],
   "source": [
    "###TEST DATA SET###\n",
    "\n",
    "\n",
    "#Normalize data\n",
    "test_df['Cycle_norm'] = test_df['Cycle']\n",
    "norm_test_df = pd.DataFrame(scaler.transform(test_df[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(norm_train_df.shape,norm_test_df.shape)\n",
    "\n",
    "#Principal Component Analysis Dimensionality Reduction\n",
    "\n",
    "transformed_test = pca.transform(norm_test_df)\n",
    "\n",
    "df_transformed_test = pd.DataFrame(transformed_test, index = test_df.index)\n",
    "\n",
    "join_df = test_df[test_df.columns.difference(cols_normalize)].join(df_transformed_test)\n",
    "test_df = join_df.reindex()\n",
    "\n",
    "# We use the ground truth dataset to generate labels for the test data.\n",
    "# generate column max for test data\n",
    "rul = pd.DataFrame(test_df.groupby('Unit')['Cycle'].max()).reset_index()\n",
    "rul.columns = ['Unit', 'max']\n",
    "RUL_test.columns = ['more']\n",
    "RUL_test['Unit'] = RUL_test.index + 1\n",
    "RUL_test['max'] = rul['max'] + RUL_test['more']\n",
    "RUL_test.drop('more', axis=1, inplace=True)\n",
    "\n",
    "# generate RUL for test data\n",
    "test_df = test_df.merge(RUL_test, on=['Unit'], how='left')\n",
    "test_df['RUL'] = test_df['max'] - test_df['Cycle']\n",
    "#test_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "#Data labeling - UL_30 - UL_50 - UL_75 --> Labeling to predict if the turbine is going to fail on les than 30, 50 or 75 cycles\n",
    "\n",
    "test_df['UL_30'] = np.where(test_df['RUL'] <= 30, 1, 0)\n",
    "test_df['UL_50'] = np.where(test_df['RUL'] <= 50, 1, 0)\n",
    "test_df['UL_75'] = np.where(test_df['RUL'] <= 75, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(15631, 50, 13)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(15631, 1)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#####\n",
    "#Data formatting#\n",
    "####\n",
    "\n",
    "# Data will be input into the RNN in windows with a certain size\n",
    "sequence_length = 50\n",
    "\n",
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "sequence_cols = list(train_df.columns[2:-5].values)\n",
    "#sequence_cols.extend([\"Cycle_norm\"])\n",
    "\n",
    "#val=list(gen_sequence(train_df[train_df['Unit']==1], sequence_length, sequence_cols))\n",
    "\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['Unit']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['Unit'].unique())\n",
    "\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "print(seq_array.shape)\n",
    "\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]] \n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['Unit']==id], sequence_length, ['RUL']) \n",
    "             for id in train_df['Unit'].unique()]\n",
    "\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_keras(y_true, y_pred):\n",
    "    ##Coefficient of Determination##\n",
    "\n",
    "    SS_res = Kback.sum(Kback.square(y_true - y_pred))\n",
    "    SS_tot = Kback.sum(Kback.square( y_true - Kback.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + Kback.epsilon()))\n",
    "\n",
    "def Computed_Score(y_true, y_pred):\n",
    "    ##Computed score used in the challenge\n",
    "\n",
    "    a1 = 10\n",
    "    a2 = 13\n",
    "    score = 0\n",
    "    d = y_pred - y_true\n",
    "\n",
    "    for i in d: \n",
    "        if i<0:\n",
    "            score += (e**(-i/a1) - 1)\n",
    "        else : \n",
    "            score += (e**(i/a2) - 1)\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_features = seq_array.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_model(init_mode = 'uniform', optim = 'ADAM',hidd_layers = 1, hidd_neurons = 10, sequence_length = 50, nb_features =13 ):\n",
    "\n",
    "    # Define the model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        input_shape = (sequence_length, nb_features),\n",
    "        units = nb_features,\n",
    "        return_sequences = True,\n",
    "        kernel_initializer= init_mode))\n",
    "    model.add(Dropout(0.1))\n",
    "    for i in range(hidd_layers):\n",
    "        model.add(LSTM(\n",
    "            units = hidd_neurons,\n",
    "            return_sequences = False,\n",
    "            kernel_initializer=init_mode))\n",
    "        model.add(Dropout(0.1))\n",
    "    model.add(Dense(units = 1))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    # Compile the model\n",
    "\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = optim,\n",
    "        metrics= ['mae', r2_keras])\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.optimizers.Adamax at 0x147af48dac8>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "keras.optimizers.get('ADAMAX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters definition\n",
    "\n",
    "batches = [50,100,200,500]\n",
    "initializers = ['uniform', 'lecun_uniform'] # 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'\n",
    "optimizers = ['adam', 'rmsprop', 'SGD','adamax', 'adadelta', 'adagrad']\n",
    "hidden_layers = [1,2]\n",
    "hidden_neurons = [7,8,9,10,11,12]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "13013.1901 - val_mae: 92.2569 - val_r2_keras: -2.4122\nEpoch 6/10\n9378/9378 [==============================] - 3s 319us/step - loss: 10150.2629 - mae: 81.7767 - r2_keras: -1.9149 - val_loss: 12925.4786 - val_mae: 91.8049 - val_r2_keras: -2.3568\nEpoch 7/10\n9378/9378 [==============================] - 3s 325us/step - loss: 10078.2820 - mae: 81.3620 - r2_keras: -1.8944 - val_loss: 12855.2133 - val_mae: 91.4433 - val_r2_keras: -2.3130\nEpoch 8/10\n9378/9378 [==============================] - 3s 338us/step - loss: 10019.0832 - mae: 81.0261 - r2_keras: -1.8812 - val_loss: 12794.8634 - val_mae: 91.1339 - val_r2_keras: -2.2759\nEpoch 9/10\n9378/9378 [==============================] - 3s 357us/step - loss: 9966.7057 - mae: 80.7179 - r2_keras: -1.8657 - val_loss: 12739.7226 - val_mae: 90.8505 - val_r2_keras: -2.2426\nEpoch 10/10\n9378/9378 [==============================] - 3s 339us/step - loss: 9922.6162 - mae: 80.4646 - r2_keras: -1.8504 - val_loss: 12688.6408 - val_mae: 90.5890 - val_r2_keras: -2.2121\n5211/5211 [==============================] - 1s 117us/step\nModel: \"sequential_20\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_39 (LSTM)               (None, 50, 13)            1404      \n_________________________________________________________________\ndropout_39 (Dropout)         (None, 50, 13)            0         \n_________________________________________________________________\nlstm_40 (LSTM)               (None, 10)                960       \n_________________________________________________________________\ndropout_40 (Dropout)         (None, 10)                0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 1)                 11        \n_________________________________________________________________\nactivation_20 (Activation)   (None, 1)                 0         \n=================================================================\nTotal params: 2,375\nTrainable params: 2,375\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 9378 samples, validate on 1043 samples\nEpoch 1/10\n9378/9378 [==============================] - 4s 461us/step - loss: 9793.1890 - mae: 82.5756 - r2_keras: -2.3038 - val_loss: 13830.3098 - val_mae: 96.4693 - val_r2_keras: -2.9608\nEpoch 2/10\n9378/9378 [==============================] - 3s 332us/step - loss: 9707.5237 - mae: 82.2077 - r2_keras: -2.2716 - val_loss: 13601.0019 - val_mae: 95.5135 - val_r2_keras: -2.9142\nEpoch 3/10\n9378/9378 [==============================] - 3s 332us/step - loss: 9516.1934 - mae: 81.1704 - r2_keras: -2.2034 - val_loss: 13427.6395 - val_mae: 94.4861 - val_r2_keras: -2.7251\nEpoch 4/10\n9378/9378 [==============================] - 3s 325us/step - loss: 9376.4111 - mae: 80.1802 - r2_keras: -2.1590 - val_loss: 13314.9175 - val_mae: 93.7832 - val_r2_keras: -2.5971\nEpoch 5/10\n9378/9378 [==============================] - 3s 319us/step - loss: 9291.3964 - mae: 79.5830 - r2_keras: -2.1344 - val_loss: 13236.2140 - val_mae: 93.3605 - val_r2_keras: -2.5416\nEpoch 6/10\n9378/9378 [==============================] - 3s 326us/step - loss: 9233.6702 - mae: 79.2033 - r2_keras: -2.1129 - val_loss: 13176.3215 - val_mae: 93.0505 - val_r2_keras: -2.5021\nEpoch 7/10\n9378/9378 [==============================] - 3s 330us/step - loss: 9184.6732 - mae: 78.9045 - r2_keras: -2.0916 - val_loss: 13125.9104 - val_mae: 92.7897 - val_r2_keras: -2.4697\nEpoch 8/10\n9378/9378 [==============================] - 3s 323us/step - loss: 9145.8106 - mae: 78.6605 - r2_keras: -2.0845 - val_loss: 13080.3800 - val_mae: 92.5557 - val_r2_keras: -2.4408\nEpoch 9/10\n9378/9378 [==============================] - 3s 328us/step - loss: 9108.2590 - mae: 78.4330 - r2_keras: -2.0694 - val_loss: 13037.8287 - val_mae: 92.3377 - val_r2_keras: -2.4141\nEpoch 10/10\n9378/9378 [==============================] - 3s 343us/step - loss: 9074.3832 - mae: 78.2327 - r2_keras: -2.0563 - val_loss: 12995.8873 - val_mae: 92.1227 - val_r2_keras: -2.3881\n5210/5210 [==============================] - 1s 101us/step\nModel: \"sequential_21\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_41 (LSTM)               (None, 50, 13)            1404      \n_________________________________________________________________\ndropout_41 (Dropout)         (None, 50, 13)            0         \n_________________________________________________________________\nlstm_42 (LSTM)               (None, 10)                960       \n_________________________________________________________________\ndropout_42 (Dropout)         (None, 10)                0         \n_________________________________________________________________\ndense_21 (Dense)             (None, 1)                 11        \n_________________________________________________________________\nactivation_21 (Activation)   (None, 1)                 0         \n=================================================================\nTotal params: 2,375\nTrainable params: 2,375\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 9378 samples, validate on 1043 samples\nEpoch 1/10\n9378/9378 [==============================] - 4s 461us/step - loss: 8674.0482 - mae: 78.2943 - r2_keras: -2.4101 - val_loss: 19354.3473 - val_mae: 115.4318 - val_r2_keras: -45.6383\nEpoch 2/10\n9378/9378 [==============================] - 3s 315us/step - loss: 8547.2363 - mae: 77.7080 - r2_keras: -2.3615 - val_loss: 19032.9806 - val_mae: 114.4144 - val_r2_keras: -44.5838\nEpoch 3/10\n9378/9378 [==============================] - 3s 310us/step - loss: 8336.6688 - mae: 76.4717 - r2_keras: -2.2819 - val_loss: 18797.1664 - val_mae: 113.3954 - val_r2_keras: -43.9428\nEpoch 4/10\n9378/9378 [==============================] - 3s 333us/step - loss: 8216.3724 - mae: 75.5308 - r2_keras: -2.2329 - val_loss: 18634.8742 - val_mae: 112.4069 - val_r2_keras: -43.5564\nEpoch 5/10\n9378/9378 [==============================] - 3s 324us/step - loss: 8129.2109 - mae: 74.8614 - r2_keras: -2.1997 - val_loss: 18520.1134 - val_mae: 111.7797 - val_r2_keras: -43.2452\nEpoch 6/10\n9378/9378 [==============================] - 3s 336us/step - loss: 8061.1733 - mae: 74.4073 - r2_keras: -2.1739 - val_loss: 18430.1680 - val_mae: 111.3829 - val_r2_keras: -42.9949\nEpoch 7/10\n9378/9378 [==============================] - 3s 350us/step - loss: 8007.5818 - mae: 74.0492 - r2_keras: -2.1554 - val_loss: 18354.1120 - val_mae: 111.0525 - val_r2_keras: -42.7824\nEpoch 8/10\n9378/9378 [==============================] - 3s 321us/step - loss: 7957.1003 - mae: 73.7299 - r2_keras: -2.1345 - val_loss: 18286.6329 - val_mae: 110.7602 - val_r2_keras: -42.5935\nEpoch 9/10\n9378/9378 [==============================] - 3s 354us/step - loss: 7918.2491 - mae: 73.4701 - r2_keras: -2.1212 - val_loss: 18224.4741 - val_mae: 110.4927 - val_r2_keras: -42.4191\nEpoch 10/10\n9378/9378 [==============================] - 3s 337us/step - loss: 7875.6364 - mae: 73.2053 - r2_keras: -2.1006 - val_loss: 18165.6336 - val_mae: 110.2397 - val_r2_keras: -42.2539\n5210/5210 [==============================] - 1s 109us/step\nModel: \"sequential_22\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_43 (LSTM)               (None, 50, 13)            1404      \n_________________________________________________________________\ndropout_43 (Dropout)         (None, 50, 13)            0         \n_________________________________________________________________\nlstm_44 (LSTM)               (None, 10)                960       \n_________________________________________________________________\ndropout_44 (Dropout)         (None, 10)                0         \n_________________________________________________________________\ndense_22 (Dense)             (None, 1)                 11        \n_________________________________________________________________\nactivation_22 (Activation)   (None, 1)                 0         \n=================================================================\nTotal params: 2,375\nTrainable params: 2,375\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 9378 samples, validate on 1042 samples\nEpoch 1/10\n9378/9378 [==============================] - 4s 459us/step - loss: 10909.2441 - mae: 85.9761 - r2_keras: -2.1357 - val_loss: 13760.0705 - val_mae: 96.0114 - val_r2_keras: -2.8830\nEpoch 2/10\n9378/9378 [==============================] - 3s 324us/step - loss: 10817.2260 - mae: 85.5475 - r2_keras: -2.1109 - val_loss: 13622.0658 - val_mae: 95.4250 - val_r2_keras: -2.8304\nEpoch 3/10\n9378/9378 [==============================] - 3s 322us/step - loss: 10698.4137 - mae: 84.9808 - r2_keras: -2.0795 - val_loss: 13460.5592 - val_mae: 94.6879 - val_r2_keras: -2.7662\nEpoch 4/10\n9378/9378 [==============================] - 3s 329us/step - loss: 10560.9284 - mae: 84.2750 - r2_keras: -2.0339 - val_loss: 13296.1808 - val_mae: 93.8919 - val_r2_keras: -2.6843\nEpoch 5/10\n9378/9378 [==============================] - 3s 335us/step - loss: 10421.9029 - mae: 83.5163 - r2_keras: -1.9929 - val_loss: 13154.9699 - val_mae: 93.1689 - val_r2_keras: -2.5924\nEpoch 6/10\n9378/9378 [==============================] - 3s 342us/step - loss: 10294.3382 - mae: 82.7758 - r2_keras: -1.9551 - val_loss: 13034.3039 - val_mae: 92.5035 - val_r2_keras: -2.4924\nEpoch 7/10\n9378/9378 [==============================] - 3s 340us/step - loss: 10185.2926 - mae: 82.1075 - r2_keras: -1.9313 - val_loss: 12936.7548 - val_mae: 91.9661 - val_r2_keras: -2.4143\nEpoch 8/10\n9378/9378 [==============================] - 3s 327us/step - loss: 10101.2642 - mae: 81.5979 - r2_keras: -1.9013 - val_loss: 12856.4442 - val_mae: 91.5338 - val_r2_keras: -2.3574\nEpoch 9/10\n9378/9378 [==============================] - 3s 331us/step - loss: 10029.0423 - mae: 81.1681 - r2_keras: -1.8811 - val_loss: 12787.8119 - val_mae: 91.1648 - val_r2_keras: -2.3114\nEpoch 10/10\n9378/9378 [==============================] - 3s 329us/step - loss: 9967.7644 - mae: 80.8026 - r2_keras: -1.8637 - val_loss: 12726.8058 - val_mae: 90.8395 - val_r2_keras: -2.2706\n5211/5211 [==============================] - 1s 108us/step\nModel: \"sequential_23\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_45 (LSTM)               (None, 50, 13)            1404      \n_________________________________________________________________\ndropout_45 (Dropout)         (None, 50, 13)            0         \n_________________________________________________________________\nlstm_46 (LSTM)               (None, 10)                960       \n_________________________________________________________________\ndropout_46 (Dropout)         (None, 10)                0         \n_________________________________________________________________\ndense_23 (Dense)             (None, 1)                 11        \n_________________________________________________________________\nactivation_23 (Activation)   (None, 1)                 0         \n=================================================================\nTotal params: 2,375\nTrainable params: 2,375\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 9378 samples, validate on 1043 samples\nEpoch 1/10\n9378/9378 [==============================] - 4s 461us/step - loss: 9800.2474 - mae: 82.4979 - r2_keras: -2.2979 - val_loss: 13821.3963 - val_mae: 96.3157 - val_r2_keras: -2.9060\nEpoch 2/10\n9378/9378 [==============================] - 3s 317us/step - loss: 9707.2895 - mae: 82.0425 - r2_keras: -2.2653 - val_loss: 13667.4380 - val_mae: 95.6669 - val_r2_keras: -2.8446\nEpoch 3/10\n9378/9378 [==============================] - 3s 334us/step - loss: 9582.4440 - mae: 81.4293 - r2_keras: -2.2277 - val_loss: 13464.7680 - val_mae: 94.7814 - val_r2_keras: -2.7677\nEpoch 4/10\n9378/9378 [==============================] - 3s 321us/step - loss: 9434.6719 - mae: 80.6477 - r2_keras: -2.1759 - val_loss: 13282.7615 - val_mae: 93.8706 - val_r2_keras: -2.6710\nEpoch 5/10\n9378/9378 [==============================] - 3s 343us/step - loss: 9267.8873 - mae: 79.6208 - r2_keras: -2.1170 - val_loss: 13119.3221 - val_mae: 92.8995 - val_r2_keras: -2.5056\nEpoch 6/10\n9378/9378 [==============================] - 3s 318us/step - loss: 9127.1795 - mae: 78.6853 - r2_keras: -2.0736 - val_loss: 12988.4129 - val_mae: 92.1982 - val_r2_keras: -2.4166\nEpoch 7/10\n9378/9378 [==============================] - 3s 323us/step - loss: 9021.6639 - mae: 78.0406 - r2_keras: -2.0351 - val_loss: 12883.3239 - val_mae: 91.6476 - val_r2_keras: -2.3507\nEpoch 8/10\n9378/9378 [==============================] - 3s 319us/step - loss: 8939.5365 - mae: 77.5330 - r2_keras: -2.0102 - val_loss: 12799.2594 - val_mae: 91.2035 - val_r2_keras: -2.2972\nEpoch 9/10\n9378/9378 [==============================] - 3s 337us/step - loss: 8871.3269 - mae: 77.1092 - r2_keras: -1.9871 - val_loss: 12728.3401 - val_mae: 90.8281 - val_r2_keras: -2.2525\nEpoch 10/10\n9378/9378 [==============================] - 3s 329us/step - loss: 8811.6135 - mae: 76.7453 - r2_keras: -1.9714 - val_loss: 12665.2295 - val_mae: 90.4951 - val_r2_keras: -2.2135\n5210/5210 [==============================] - 0s 93us/step\nModel: \"sequential_24\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_47 (LSTM)               (None, 50, 13)            1404      \n_________________________________________________________________\ndropout_47 (Dropout)         (None, 50, 13)            0         \n_________________________________________________________________\nlstm_48 (LSTM)               (None, 10)                960       \n_________________________________________________________________\ndropout_48 (Dropout)         (None, 10)                0         \n_________________________________________________________________\ndense_24 (Dense)             (None, 1)                 11        \n_________________________________________________________________\nactivation_24 (Activation)   (None, 1)                 0         \n=================================================================\nTotal params: 2,375\nTrainable params: 2,375\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 9378 samples, validate on 1043 samples\nEpoch 1/10\n9378/9378 [==============================] - 4s 450us/step - loss: 8683.8960 - mae: 78.2535 - r2_keras: -2.4151 - val_loss: 19359.3180 - val_mae: 115.3385 - val_r2_keras: -45.7344\nEpoch 2/10\n9378/9378 [==============================] - 3s 303us/step - loss: 8630.2090 - mae: 77.9846 - r2_keras: -2.4030 - val_loss: 19289.7180 - val_mae: 115.0912 - val_r2_keras: -45.6155\nEpoch 3/10\n9378/9378 [==============================] - 3s 297us/step - loss: 8576.9364 - mae: 77.6918 - r2_keras: -2.3730 - val_loss: 19203.6562 - val_mae: 114.7572 - val_r2_keras: -45.3374\nEpoch 4/10\n9378/9378 [==============================] - 3s 292us/step - loss: 8511.8149 - mae: 77.2999 - r2_keras: -2.3486 - val_loss: 19091.6700 - val_mae: 114.3006 - val_r2_keras: -44.8837\nEpoch 5/10\n9378/9378 [==============================] - 3s 311us/step - loss: 8432.8153 - mae: 76.8022 - r2_keras: -2.3150 - val_loss: 18961.2840 - val_mae: 113.7491 - val_r2_keras: -44.4792\nEpoch 6/10\n9378/9378 [==============================] - 3s 314us/step - loss: 8338.9365 - mae: 76.2097 - r2_keras: -2.2768 - val_loss: 18807.5501 - val_mae: 113.1011 - val_r2_keras: -44.0630\nEpoch 7/10\n9378/9378 [==============================] - 3s 289us/step - loss: 8233.7750 - mae: 75.5339 - r2_keras: -2.2438 - val_loss: 18649.8692 - val_mae: 112.4107 - val_r2_keras: -43.6301\nEpoch 8/10\n9378/9378 [==============================] - 3s 310us/step - loss: 8138.4537 - mae: 74.9115 - r2_keras: -2.1987 - val_loss: 18518.7317 - val_mae: 111.8185 - val_r2_keras: -43.2534\nEpoch 9/10\n9378/9378 [==============================] - 3s 294us/step - loss: 8059.2340 - mae: 74.4020 - r2_keras: -2.1679 - val_loss: 18415.1652 - val_mae: 111.3629 - val_r2_keras: -42.9472\nEpoch 10/10\n9378/9378 [==============================] - 3s 298us/step - loss: 7996.3041 - mae: 74.0047 - r2_keras: -2.1472 - val_loss: 18330.7688 - val_mae: 110.9908 - val_r2_keras: -42.7102\n5210/5210 [==============================] - 1s 107us/step\nModel: \"sequential_25\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_49 (LSTM)               (None, 50, 13)            1404      \n_________________________________________________________________\ndropout_49 (Dropout)         (None, 50, 13)            0         \n_________________________________________________________________\nlstm_50 (LSTM)               (None, 10)                960       \n_________________________________________________________________\ndropout_50 (Dropout)         (None, 10)                0         \n_________________________________________________________________\ndense_25 (Dense)             (None, 1)                 11        \n_________________________________________________________________\nactivation_25 (Activation)   (None, 1)                 0         \n=================================================================\nTotal params: 2,375\nTrainable params: 2,375\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 14067 samples, validate on 1564 samples\nEpoch 1/10\n14067/14067 [==============================] - 16s 1ms/step - loss: 9228.1327 - mae: 78.3075 - r2_keras: -2.0521 - val_loss: 11965.1427 - val_mae: 88.1762 - val_r2_keras: -41.5911\nEpoch 2/10\n14067/14067 [==============================] - 16s 1ms/step - loss: 8543.9480 - mae: 74.1530 - r2_keras: -1.8167 - val_loss: 11396.5222 - val_mae: 85.2476 - val_r2_keras: -39.5028\nEpoch 3/10\n14067/14067 [==============================] - 22s 2ms/step - loss: 8090.0213 - mae: 71.4256 - r2_keras: -1.6684 - val_loss: 10885.3284 - val_mae: 82.5981 - val_r2_keras: -37.6582\nEpoch 4/10\n14067/14067 [==============================] - 18s 1ms/step - loss: 7671.3617 - mae: 68.8098 - r2_keras: -1.5291 - val_loss: 10403.3171 - val_mae: 79.8893 - val_r2_keras: -35.7500\nEpoch 5/10\n14067/14067 [==============================] - 19s 1ms/step - loss: 7269.1452 - mae: 66.0520 - r2_keras: -1.3917 - val_loss: 9943.7681 - val_mae: 77.1137 - val_r2_keras: -34.0255\nEpoch 6/10\n14067/14067 [==============================] - 15s 1ms/step - loss: 6897.6068 - mae: 63.4665 - r2_keras: -1.2667 - val_loss: 9510.4695 - val_mae: 74.6738 - val_r2_keras: -32.4115\nEpoch 7/10\n14067/14067 [==============================] - 20s 1ms/step - loss: 6542.9595 - mae: 61.0555 - r2_keras: -1.1549 - val_loss: 9096.3398 - val_mae: 72.3078 - val_r2_keras: -30.8616\nEpoch 8/10\n14067/14067 [==============================] - 25s 2ms/step - loss: 6208.8326 - mae: 58.7559 - r2_keras: -1.0375 - val_loss: 8700.0601 - val_mae: 69.8587 - val_r2_keras: -29.3979\nEpoch 9/10\n14067/14067 [==============================] - 24s 2ms/step - loss: 5899.2422 - mae: 56.5775 - r2_keras: -0.9332 - val_loss: 8324.7935 - val_mae: 67.8133 - val_r2_keras: -28.0293\nEpoch 10/10\n14067/14067 [==============================] - 20s 1ms/step - loss: 5589.5707 - mae: 54.4449 - r2_keras: -0.8210 - val_loss: 7961.5933 - val_mae: 65.5275 - val_r2_keras: -26.6666\n"
    }
   ],
   "source": [
    "model_CV = KerasRegressor(build_fn=hyp_model, verbose = 1, epochs = 10, validation_split = 0.1)\n",
    "\n",
    "# Grid Search for batches, initializer, optimizer, hidden layers and neurons.\n",
    "\n",
    "param_grid = dict(init_mode = initializers, batch_size = batches)\n",
    "\n",
    "grid = GridSearchCV(estimator = model_CV, param_grid=param_grid, cv = 3)\n",
    "\n",
    "grid_result = grid.fit(seq_array, label_array, \n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience=10, verbose = 1, mode = 'min'), \n",
    "        keras.callbacks.ModelCheckpoint(model_path, monitor = 'val_loss', save_best_only = True, mode = 'min', verbose = 0)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Best Accuracy for -6.844e+03 using {'batch_size': 50, 'init_mode': 'uniform'}\nmean=-6.844e+03, std=1.034e+03 using {'batch_size': 50, 'init_mode': 'uniform'}\nmean=-6.85e+03, std=995.1 using {'batch_size': 50, 'init_mode': 'lecun_uniform'}\nmean=-8.062e+03, std=1.038e+03 using {'batch_size': 100, 'init_mode': 'uniform'}\nmean=-8.081e+03, std=1.245e+03 using {'batch_size': 100, 'init_mode': 'lecun_uniform'}\nmean=-8.943e+03, std=1.223e+03 using {'batch_size': 200, 'init_mode': 'uniform'}\nmean=-8.933e+03, std=1.235e+03 using {'batch_size': 200, 'init_mode': 'lecun_uniform'}\nmean=-9.508e+03, std=1.234e+03 using {'batch_size': 500, 'init_mode': 'uniform'}\nmean=-9.469e+03, std=1.235e+03 using {'batch_size': 500, 'init_mode': 'lecun_uniform'}\n"
    }
   ],
   "source": [
    "# print results\n",
    "print(f'Best Accuracy for {grid_result.best_score_:.4} using {grid_result.best_params_}')\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'mean={mean:.4}, std={stdev:.4} using {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitumapconda899e4e23b45c45648e36a6efed0c4038",
   "display_name": "Python 3.6.10 64-bit ('UMAP': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}